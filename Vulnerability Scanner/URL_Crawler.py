import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from session_manager import session

visited_urls = set()
forms_discovered = []
target_url = ""

def crawl_site(url):

    if url in visited_urls:
        return
    # Add the URL to visited set
    visited_urls.add(url)
    print("Crawling: " + str(url))

    # When an error occurs

    try:
        # Get request for URL
        URL_response = session.get(url, timeout=5)
        URL_response.raise_for_status()

    # Access exception error class
    except requests.exceptions.RequestException as errorMessage:
        print("Request fail for " + str(url) + " - Error: " + str(errorMessage))
        return

    # Parse through the HTML content of the page
    URL_content = BeautifulSoup(URL_response.content, 'html.parser')

    # Find forms within URL
    page_forms = URL_content.find_all('form')
    for form in page_forms:

        form_details = {}

        # Gather form's main attributes
        action = form.attrs.get("action", "").lower()
        method = form.attrs.get("method", "get").lower()

        inputs = []

        # Gather form's inputs
        for input_tag in form.find_all("input"):
            input_type = input_tag.attrs.get("type", "text")
            input_name = input_tag.attrs.get("name")
            input_value = input_tag.attrs.get("value", "")
            inputs.append({"type": input_type, "name": input_name, "value": input_value})
            
        form_details["action"] = urljoin(url, action)
        form_details["method"] = method
        form_details["inputs"] = inputs
        form_details["source_url"] = url

        # Add report to master list
        forms_discovered.append(form_details)

    # Find all anchor (<a>) tags which contain links
    for a in URL_content.find_all('a', href=True):
        # Join the found link with the base URL to handle relative paths
        link = urljoin(url, a['href'])

        # Crawl links that are part of the target website
        if target_url in link and link not in visited_urls:
            crawl_site(link)